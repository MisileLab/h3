# Hugging Face Token (optional, for private models)
HUGGINGFACE_TOKEN=your_huggingface_token_here

# Weights & Biases API Key (optional, for experiment tracking)
WANDB_API_KEY=your_wandb_api_key_here

# WandB project name
WANDB_PROJECT=segment-jailbreak-detection

# WandB entity name (optional, defaults to your username)
WANDB_ENTITY=your_wandb_entity_here

# Model cache directory
HF_HOME=~/.cache/huggingface

# Data directory
DATA_DIR=./data

# Experiment output directory
EXPERIMENT_DIR=./experiments

# Model checkpoint directory
MODEL_DIR=./models

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Maximum sequence length for tokenization
MAX_LENGTH=512

# Batch size for training/inference
BATCH_SIZE=32

# Learning rate
LEARNING_RATE=2e-5

# Number of training epochs
NUM_EPOCHS=3

# Early stopping patience
EARLY_STOPPING_PATIENCE=3

# Early stopping threshold
EARLY_STOPPING_THRESHOLD=0.001

# LoRA rank (for parameter-efficient fine-tuning)
LORA_RANK=16

# LoRA alpha
LORA_ALPHA=32

# LoRA dropout
LORA_DROPOUT=0.1

# Random seed for reproducibility
SEED=42

# CUDA device (if multiple GPUs available)
CUDA_VISIBLE_DEVICES=0