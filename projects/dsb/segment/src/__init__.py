"""
ELECTRA-based Prompt Jailbreak Guard Model

A lightweight safety classifier for detecting and preventing prompt injection
and jailbreak attacks on Large Language Models.
"""

__version__ = "0.1.0"
__author__ = "Your Name"