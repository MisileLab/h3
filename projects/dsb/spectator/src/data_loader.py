"""Data loader for fetching ETH OHLCV and news data from CoinDesk API."""

import json
import requests
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, final

import polars as pl
from tqdm import tqdm

from .config import (
  COINDESK_BASE_URL,
  COINDESK_API_KEY,
  RAW_DATA_DIR,
  PRICE_LOOKBACK_DAYS,
  NEWS_LOOKBACK_DAYS,
  NEWS_BATCH_SIZE,
  PRICE_BATCH_DAYS,
  PRICE_DATA_FILE,
  NEWS_DATA_FILE,
  PRICE_DATA_JSON,
  NEWS_DATA_JSON,
)


@final
class CoinDeskDataLoader:
  """Fetches ETH OHLCV and news data from CoinDesk API."""

  def __init__(self) -> None:
    """Initialize the data loader."""
    self.session = requests.Session()
    headers = {
      "Authorization": f"Bearer {COINDESK_API_KEY}"
    }
    
    self.session.headers.update(headers)

  def fetch_price_data(self) -> pl.DataFrame:
    """
    Fetch daily OHLCV data for ETH from CoinDesk Data API using sliding window approach.
    Fetches price data going back 1 year using date-based batching.
    
    Returns:
      DataFrame with columns: date, open, high, low, close, volume
    """
    end_date = datetime.now(tz=timezone.utc)
    start_date = end_date - timedelta(days=PRICE_LOOKBACK_DAYS)
    
    print(f"Fetching price data from {start_date.date()} to {end_date.date()}")
    
    all_price_data = []
    current_end = end_date
    
    # CoinDesk Data API endpoint for historical spot data
    url = f"{COINDESK_BASE_URL}/spot/v1/historical/days"
    
    try:
      with tqdm(desc="Fetching price batches") as pbar:
        while current_end.date() > start_date.date():
          # Calculate batch start date (go back PRICE_BATCH_DAYS from current_end)
          batch_start = current_end - timedelta(days=PRICE_BATCH_DAYS)
          if batch_start < start_date:
            batch_start = start_date - timedelta(days=1)
          
          params = {
            "market": "binance",
            "instrument": "ETH-USDT",
            "start_date": batch_start.strftime("%Y-%m-%d"),
            "end_date": current_end.strftime("%Y-%m-%d"),
            "interval": "1d",
            "format": "json"
          }
          
          response = self.session.get(url, params=params, timeout=30)
          response.raise_for_status()
          data = response.json()
          
          batch_prices = []
          
          if "Data" in data and data["Data"]:
            for bar in data["Data"]:
              batch_prices.append({
                "date": datetime.fromtimestamp(bar["TIMESTAMP"], tz=timezone.utc).date(),
                "open": float(bar["OPEN"]),
                "high": float(bar["HIGH"]),
                "low": float(bar["LOW"]),
                "close": float(bar["CLOSE"]),
                "volume": float(bar.get("VOLUME", 0)),
              })
            
            all_price_data.extend(batch_prices)
            
            pbar.set_postfix({
              "days": len(all_price_data),
              "latest_date": current_end.date(),
              "earliest_date": batch_start.date()
            })
            pbar.update(1)
          else:
            # No data in this batch, move to next
            print(f"No price data for batch {batch_start.date()} to {current_end.date()}")
          
          # Move to next batch (go back PRICE_BATCH_DAYS)
          current_end = batch_start
          
          # Rate limiting - small delay between requests
          import time
          time.sleep(0.1)
      
      # If no data was fetched from API, use fallback simulation
      if not all_price_data:
        print("Warning: Using simulated price data - API may be unavailable or format changed")
        all_price_data = self._generate_simulated_prices(start_date, end_date)
      
      # Create DataFrame and remove duplicates
      df = pl.DataFrame(all_price_data)
      
      if len(df) > 0:
        # Remove duplicate dates (keep first occurrence)
        df = df.unique(subset=["date"], keep="first")
        df = df.sort("date")
      
      print(f"Fetched {len(df)} days of price data")
      
      # Save raw data in both formats
      self._save_dataframe(df, "price")
      
      return df
      
    except requests.RequestException as e:
      print(f"Price API request failed, using simulated data: {e}")
      # Fallback to simulated data
      simulated_data = self._generate_simulated_prices(start_date, end_date)
      df = pl.DataFrame(simulated_data)
      self._save_dataframe(df, "price")
      return df
    except (KeyError, ValueError) as e:
      raise RuntimeError(f"Failed to parse price data from CoinDesk API: {e}")

  def _generate_simulated_prices(self, start_date: datetime, end_date: datetime) -> list[dict[str, Any]]:
    """Generate simulated price data as fallback."""
    import random
    
    prices = []
    current_date = start_date
    base_price = 2000.0  # ETH base price
    
    random.seed(42)  # Reproducible simulation
    
    while current_date <= end_date:
      # Simulate realistic price movements
      daily_change = random.uniform(-0.05, 0.05)  # Â±5% daily change
      close_price = base_price * (1 + daily_change)
      
      # Generate realistic OHLC from open and close
      open_price = base_price
      high_price = max(open_price, close_price) * (1 + random.uniform(0, 0.02))
      low_price = min(open_price, close_price) * (1 - random.uniform(0, 0.02))
      volume = random.uniform(50000, 200000)
      
      prices.append({
        "date": current_date.date(),
        "open": open_price,
        "high": high_price,
        "low": low_price,
        "close": close_price,
        "volume": volume,
      })
      
      base_price = close_price
      current_date += timedelta(days=1)
    
    return prices

  def fetch_news_data(self) -> pl.DataFrame:
    """
    Fetch ETH-related news data from CoinDesk Data API using sliding window approach.
    Fetches news going back 1 year using timestamp-based pagination.
    
    Returns:
      DataFrame with columns: date, headline, content, url
    """
    end_date = datetime.now(tz=timezone.utc)
    start_date = end_date - timedelta(days=NEWS_LOOKBACK_DAYS)
    
    print(f"Fetching news from {start_date.date()} to {end_date.date()}")
    
    all_news_data = []
    current_to_ts = int(end_date.timestamp())
    target_from_ts = int(start_date.timestamp())
    
    # CoinDesk Data API endpoint for news search
    url = f"{COINDESK_BASE_URL}/news/v1/search"
    
    try:
      with tqdm(desc="Fetching news batches") as pbar:
        while current_to_ts > target_from_ts:
          params = {
            "search_string": "ethereum",
            "to_ts": current_to_ts,
            "limit": NEWS_BATCH_SIZE,
            "sort": "published_at",
            "order": "desc",
            "format": "json",
            "source_key": "coindesk"
          }
          
          response = self.session.get(url, params=params, timeout=30)
          response.raise_for_status()
          data = response.json()
          
          batch_news = []
          earliest_ts = current_to_ts
          
          if "Data" in data and data["Data"]:
            for article in data["Data"]:
              # Parse publication timestamp
              published_ts = article.get("PUBLISHED_ON", 0)
              if published_ts < target_from_ts:
                # Reached our target date, stop here
                break
              
              pub_date = datetime.fromtimestamp(published_ts, tz=timezone.utc).date()
              
              batch_news.append({
                "date": pub_date,
                "headline": article.get("TITLE", ""),
                "content": article.get("BODY", ""),
                "url": article.get("URL", ""),
                "published_ts": published_ts,
              })
              
              earliest_ts = min(earliest_ts, published_ts)
            
            all_news_data.extend(batch_news)
            
            # Update pagination cursor - move to timestamp before earliest article
            if earliest_ts < current_to_ts:
              current_to_ts = earliest_ts - 1
            else:
              # No new articles found, break to avoid infinite loop
              break
            
            pbar.set_postfix({
              "articles": len(all_news_data),
              "latest_date": datetime.fromtimestamp(earliest_ts, tz=timezone.utc).date()
            })
            pbar.update(1)
          else:
            # No more data available
            break
          
          # Rate limiting - small delay between requests
          import time
          time.sleep(0.1)
      
      # If no data was fetched from API, use fallback simulation
      if not all_news_data:
        print("Warning: Using simulated news data - API may be unavailable or format changed")
        all_news_data = self._generate_simulated_news(start_date, end_date)
      
      # Create DataFrame and process
      df = pl.DataFrame(all_news_data)
      
      if len(df) > 0:
        # Remove duplicate timestamp column if it exists
        if "published_ts" in df.columns:
          df = df.drop("published_ts")
        
        # Keep one article per day, preferring longer content
        df = df.with_columns([
          pl.col("content").str.len_chars().alias("content_length")
        ])
        df = df.sort(["date", "content_length"], descending=[False, True])
        df = df.group_by("date").first()
        df = df.drop("content_length")
        df = df.sort("date")
      
      print(f"Fetched {len(df)} days of news data")
      
      # Save raw data in both formats
      self._save_dataframe(df, "news")
      
      return df
      
    except requests.RequestException as e:
      print(f"API request failed, using simulated data: {e}")
      # Fallback to simulated data
      simulated_data = self._generate_simulated_news(start_date, end_date)
      df = pl.DataFrame(simulated_data)
      self._save_dataframe(df, "news")
      return df
    except (KeyError, ValueError) as e:
      raise RuntimeError(f"Failed to parse news data from CoinDesk API: {e}")

  def _generate_simulated_news(self, start_date: datetime, end_date: datetime) -> list[dict[str, Any]]:
    """Generate simulated news data as fallback."""
    news_data = []
    current_date = start_date
    
    while current_date <= end_date:
      # Generate realistic ETH news headlines and content
      headlines = [
        f"Ethereum Price Analysis: Market Shows {['Bullish', 'Bearish', 'Mixed'][current_date.day % 3]} Signals",
        f"ETH Network Activity {['Surges', 'Declines', 'Stabilizes'][current_date.day % 3]} as DeFi Adoption Continues",
        f"Institutional Interest in Ethereum {['Grows', 'Wanes', 'Remains Steady'][current_date.day % 3]}",
        f"Ethereum 2.0 Updates: {['Progress', 'Challenges', 'Milestones'][current_date.day % 3]} in Development",
      ]
      
      headline = headlines[current_date.day % len(headlines)]
      content = f"Market analysis for {current_date.strftime('%Y-%m-%d')}: " \
               f"Ethereum continues to show significant market activity with various factors " \
               f"influencing price movements. Technical indicators suggest mixed signals " \
               f"while fundamental developments in the ecosystem remain strong. Trading " \
               f"volumes and network utilization provide insights into market sentiment."
      
      news_data.append({
        "date": current_date.date(),
        "headline": headline,
        "content": content,
        "url": f"https://coindesk.com/markets/ethereum-{current_date.strftime('%Y-%m-%d')}",
      })
      current_date += timedelta(days=1)
    
    return news_data

  def _save_json_data(self, data: list[dict[str, Any]], filepath: Path) -> None:
    """Save data to JSON file."""
    with open(filepath, "w", encoding="utf-8") as f:
      json.dump(data, f, indent=2, default=str)

  def _save_dataframe(self, df: pl.DataFrame, data_type: str) -> None:
    """
    Save dataframe in both Parquet (efficient) and JSON (backup) formats.
    
    Args:
      df: DataFrame to save
      data_type: Type of data ('price' or 'news')
    """
    if data_type == "price":
      parquet_file = RAW_DATA_DIR / PRICE_DATA_FILE
      json_file = RAW_DATA_DIR / PRICE_DATA_JSON
    elif data_type == "news":
      parquet_file = RAW_DATA_DIR / NEWS_DATA_FILE
      json_file = RAW_DATA_DIR / NEWS_DATA_JSON
    else:
      raise ValueError(f"Unknown data_type: {data_type}")
    
    try:
      # Save as Parquet (efficient format for Polars)
      df.write_parquet(parquet_file)
      print(f"Saved {data_type} data to {parquet_file}")
      
      # Save as JSON (backup/human-readable format)
      self._save_json_data(df.to_dicts(), json_file)
      print(f"Saved {data_type} data backup to {json_file}")
      
    except Exception as e:
      print(f"Warning: Failed to save {data_type} data to Parquet, using JSON only: {e}")
      # Fallback to JSON only
      self._save_json_data(df.to_dicts(), json_file)

  def _load_dataframe(self, data_type: str) -> pl.DataFrame | None:
    """
    Load dataframe from Parquet or JSON format.
    
    Args:
      data_type: Type of data ('price' or 'news')
      
    Returns:
      DataFrame if found, None otherwise
    """
    if data_type == "price":
      parquet_file = RAW_DATA_DIR / PRICE_DATA_FILE
      json_file = RAW_DATA_DIR / PRICE_DATA_JSON
    elif data_type == "news":
      parquet_file = RAW_DATA_DIR / NEWS_DATA_FILE
      json_file = RAW_DATA_DIR / NEWS_DATA_JSON
    else:
      raise ValueError(f"Unknown data_type: {data_type}")
    
    # Try Parquet first (more efficient)
    if parquet_file.exists():
      try:
        df = pl.read_parquet(parquet_file)
        print(f"Loaded {data_type} data from {parquet_file}")
        return df
      except Exception as e:
        print(f"Warning: Failed to load {data_type} data from Parquet: {e}")
    
    # Fallback to JSON
    if json_file.exists():
      try:
        with open(json_file, encoding="utf-8") as f:
          data = json.load(f)
        df = pl.DataFrame(data)
        df = df.with_columns(pl.col("date").str.to_date())
        print(f"Loaded {data_type} data from {json_file}")
        return df
      except Exception as e:
        print(f"Warning: Failed to load {data_type} data from JSON: {e}")
    
    return None

  def load_cached_data(self) -> tuple[pl.DataFrame | None, pl.DataFrame | None]:
    """
    Load cached data if available.
    
    Returns:
      Tuple of (price_df, news_df) or (None, None) if not cached
    """
    price_df = self._load_dataframe("price")
    news_df = self._load_dataframe("news")
    
    return price_df, news_df

  def save_processed_data(self, windows: list[dict[str, Any]], filename: str | None = None) -> None:
    """
    Save processed window data to safetensors format.
    
    Args:
      windows: List of processed windows with embeddings
      filename: Optional custom filename (defaults to PROCESSED_DATA_FILE)
    """
    import numpy as np
    from safetensors.numpy import save_file
    from .config import PROCESSED_DATA_FILE
    
    if not windows:
      print("No windows to save")
      return
    
    filepath = RAW_DATA_DIR / (filename or PROCESSED_DATA_FILE)
    
    try:
      # Convert windows to arrays for safetensors
      tensors = {}
      
      # Extract arrays from windows
      dates = [str(w["date"]) for w in windows]
      price_sequences = np.array([w["price_sequence"] for w in windows], dtype=np.float32)
      news_embeddings = np.array([w.get("news_embedding", np.zeros(1536)) for w in windows], dtype=np.float32)
      directions = np.array([w["direction"] for w in windows], dtype=np.int32)
      volatilities = np.array([w["volatility"] for w in windows], dtype=np.float32)
      
      # Store in tensors dict
      tensors["price_sequences"] = price_sequences
      tensors["news_embeddings"] = news_embeddings  
      tensors["directions"] = directions
      tensors["volatilities"] = volatilities
      
      # Save to safetensors
      save_file(tensors, str(filepath))
      
      # Save metadata (dates) to JSON
      metadata_file = filepath.with_suffix(".json")
      with open(metadata_file, "w", encoding="utf-8") as f:
        json.dump({"dates": dates, "num_windows": len(windows)}, f, indent=2)
      
      print(f"Saved {len(windows)} processed windows to {filepath}")
      print(f"Saved metadata to {metadata_file}")
      
    except Exception as e:
      print(f"Failed to save processed data: {e}")

  def load_processed_data(self, filename: str | None = None) -> list[dict[str, Any]] | None:
    """
    Load processed window data from safetensors format.
    
    Args:
      filename: Optional custom filename (defaults to PROCESSED_DATA_FILE)
      
    Returns:
      List of processed windows or None if not found
    """
    from safetensors.numpy import load_file
    from .config import PROCESSED_DATA_FILE
    
    filepath = RAW_DATA_DIR / (filename or PROCESSED_DATA_FILE)
    metadata_file = filepath.with_suffix(".json")
    
    if not filepath.exists() or not metadata_file.exists():
      return None
    
    try:
      # Load tensors
      tensors = load_file(str(filepath))
      
      # Load metadata
      with open(metadata_file, encoding="utf-8") as f:
        metadata = json.load(f)
      
      dates = metadata["dates"]
      num_windows = metadata["num_windows"]
      
      # Reconstruct windows
      windows = []
      for i in range(num_windows):
        window = {
          "date": dates[i],
          "price_sequence": tensors["price_sequences"][i].tolist(),
          "news_embedding": tensors["news_embeddings"][i],
          "direction": int(tensors["directions"][i]),
          "volatility": float(tensors["volatilities"][i]),
        }
        windows.append(window)
      
      print(f"Loaded {len(windows)} processed windows from {filepath}")
      return windows
      
    except Exception as e:
      print(f"Failed to load processed data: {e}")
      return None

  def fetch_all_data(self, use_cache: bool = True) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Fetch both price and news data.
    
    Args:
      use_cache: Whether to use cached data if available
      
    Returns:
      Tuple of (price_df, news_df)
    """
    if use_cache:
      price_df, news_df = self.load_cached_data()
      if price_df is not None and news_df is not None:
        print("Using cached data")
        return price_df, news_df
    
    print("Fetching fresh data from APIs...")
    
    with tqdm(total=2, desc="Fetching data") as pbar:
      pbar.set_description("Fetching price data")
      price_df = self.fetch_price_data()
      pbar.update(1)
      
      pbar.set_description("Fetching news data")
      news_df = self.fetch_news_data()
      pbar.update(1)
    
    return price_df, news_df


def main() -> None:
  """Test the data loader."""
  loader = CoinDeskDataLoader()
  price_df, news_df = loader.fetch_all_data(use_cache=False)
  
  print(f"Price data shape: {price_df.shape}")
  print(f"News data shape: {news_df.shape}")
  print("\nPrice data sample:")
  print(price_df.head())
  print("\nNews data sample:")
  print(news_df.head())


if __name__ == "__main__":
  main()