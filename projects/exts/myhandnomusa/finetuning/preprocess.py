
import polars as pl
import os
import asyncio
import get_laws_api
import get_precedents_api

from datasets import Dataset, DatasetDict

# --- Configuration ---
HUB_DATASET_ID = "misilelab/korean-law-dataset"
# Input files (will be generated by the API scripts)
LAW_FILE = "korean_labor_laws.parquet"
PRECEDENT_FILE = "korean_labor_precedents_with_content.parquet"

# Output directory and files
OUTPUT_DIR = "./processed_data"
TRAIN_FILE = os.path.join(OUTPUT_DIR, "train.parquet")
VALIDATION_FILE = os.path.join(OUTPUT_DIR, "validation.parquet")
TEST_FILE = os.path.join(OUTPUT_DIR, "test.parquet")

# Split ratios
TRAIN_RATIO = 0.8
VALIDATION_RATIO = 0.1
# TEST_RATIO is the remainder

# Seed for reproducibility
RANDOM_SEED = 42

async def run_data_collection():
    """Runs the data collection scripts to ensure data is up-to-date."""
    print("--- Running data collection scripts ---")
    try:
        await get_laws_api.main()
        await get_precedents_api.main()
        print("--- Data collection finished ---")
        return True
    except Exception as e:
        print(f"An error occurred during data collection: {e}")
        return False

def main():
    """Main preprocessing function to load, clean, combine, and split the data."""
    MIN_TEXT_LENGTH = 50

    # Create output directory if it doesn't exist
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- Load and Clean Data ---
    print("--- Loading and Cleaning Data ---")
    data_sources = []
    
    # Process Laws
    try:
        print(f"Loading law data from {LAW_FILE}...")
        laws_df = pl.read_parquet(LAW_FILE).rename({"본문": "text"}).select(["text"])
        laws_df = laws_df.filter(pl.col("text").is_not_null() & (pl.col("text").str.len_chars() >= MIN_TEXT_LENGTH))
        laws_df = laws_df.with_columns(pl.lit("law").alias("source"))
        stats = laws_df.select(pl.col("text").str.len_chars().alias("length")).describe()
        print(f"Loaded and cleaned {len(laws_df)} law documents.")
        print("Law text length stats:\n", stats)
        data_sources.append(laws_df)
    except Exception as e:
        print(f"Error processing law data: {e}")
        return

    # Process Precedents
    try:
        print(f"Loading precedent data from {PRECEDENT_FILE}...")
        precedents_df = pl.read_parquet(PRECEDENT_FILE).rename({"판례내용": "text"}).select(["text"])
        precedents_df = precedents_df.filter(pl.col("text").is_not_null() & (pl.col("text").str.len_chars() >= MIN_TEXT_LENGTH))
        precedents_df = precedents_df.with_columns(pl.lit("precedent").alias("source"))
        stats = precedents_df.select(pl.col("text").str.len_chars().alias("length")).describe()
        print(f"Loaded and cleaned {len(precedents_df)} precedent documents.")
        print("Precedent text length stats:\n", stats)
        data_sources.append(precedents_df)
    except Exception as e:
        print(f"Error processing precedent data: {e}")
        return

    # --- Combine Data ---
    if not data_sources:
        print("No data loaded. Exiting.")
        return
    combined_df = pl.concat(data_sources)
    
    source_counts = combined_df.group_by("source").agg(pl.len().alias("count"))
    print("\n--- Data Distribution ---")
    print(source_counts)
    print(f"Total combined documents: {len(combined_df)}")


    # --- Stratified Split ---
    print("\n--- Performing Stratified Split ---")
    
    # Group by source
    grouped = combined_df.group_by('source', maintain_order=True)

    train_parts, val_parts, test_parts = [], [], []

    # For each group, sample and split
    for _, group_df in grouped:
        group_df = group_df.sample(fraction=1, shuffle=True, seed=RANDOM_SEED)
        n = len(group_df)
        train_size = int(n * TRAIN_RATIO)
        val_size = int(n * VALIDATION_RATIO)

        train_parts.append(group_df.slice(0, train_size))
        val_parts.append(group_df.slice(train_size, train_size + val_size))
        test_parts.append(group_df.slice(train_size + val_size))

    # Concatenate the parts for each set and shuffle again
    train_df = pl.concat(train_parts).sample(fraction=1, shuffle=True, seed=RANDOM_SEED)
    validation_df = pl.concat(val_parts).sample(fraction=1, shuffle=True, seed=RANDOM_SEED)
    test_df = pl.concat(test_parts).sample(fraction=1, shuffle=True, seed=RANDOM_SEED)


    # --- Save Data ---
    print(f"\nSaving datasets to {OUTPUT_DIR}...")
    train_df.write_parquet(TRAIN_FILE)
    validation_df.write_parquet(VALIDATION_FILE)
    test_df.write_parquet(TEST_FILE)

    print("\nPreprocessing complete!")
    print(f"  - Training set:   {len(train_df)} rows ({TRAIN_FILE})")
    print(f"  - Validation set: {len(validation_df)} rows ({VALIDATION_FILE})")
    print(f"  - Test set:       {len(test_df)} rows ({TEST_FILE})")

    # --- Push to Hugging Face Hub ---
    print(f"\n--- Pushing dataset to Hugging Face Hub ---")
    try:
        # Convert polars DataFrames to Hugging Face Datasets
        hf_train = Dataset.from_pandas(train_df.to_pandas())
        hf_validation = Dataset.from_pandas(validation_df.to_pandas())
        hf_test = Dataset.from_pandas(test_df.to_pandas())

        # Create a DatasetDict
        dataset_dict = DatasetDict({
            "train": hf_train,
            "validation": hf_validation,
            "test": hf_test
        })

        # Push to the Hub
        print(f"Pushing to repository: {HUB_DATASET_ID}")
        dataset_dict.push_to_hub(HUB_DATASET_ID, private=False) # Set private=True if you want
        print("Dataset pushed successfully!")

    except Exception as e:
        print(f"An error occurred while pushing to the Hub: {e}")
        print("Please ensure you are logged in via 'huggingface-cli login' and have write permissions.")

if __name__ == "__main__":
    # First, ensure the data is downloaded
    if asyncio.run(run_data_collection()):
        # Then, run the preprocessing
        main()
